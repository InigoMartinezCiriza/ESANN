{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29e4506",
   "metadata": {},
   "source": [
    "## Economic Choice Partial Environment RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff85ab1",
   "metadata": {},
   "source": [
    "In this notebook, the RNN agent is trained for the partially observed states environment. As learning is tough for this environment, there the number of time steps is increased progresivelly until the original times are reached (1.5 seconds for Fixation, 1-2 seconds for Offer and 2 seconds for Decision). To ensure a steady learning, the learning rate is decreased as the number of episodes increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- GPU Configuration ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# --- Add Modules directory to Python path ---\n",
    "module_path = os.path.abspath(os.path.join('..', 'Modules'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added '{module_path}' to sys.path\")\n",
    "else:\n",
    "    print(f\"'{module_path}' already in sys.path\")\n",
    "\n",
    "# --- Import custom modules ---\n",
    "from env_economic_choice_partial import EconomicChoiceEnv\n",
    "from actor_critic import ActorCriticAgent\n",
    "from reinforce import train_agent\n",
    "from helper_functions import load_model, save_model\n",
    "\n",
    "# --- Set Random Seeds ---\n",
    "seed_value = 1\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96262477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.0,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "        \n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, \\\n",
    "    actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config[\"num_episodes\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        print_interval=config[\"print_interval\"],\n",
    "        l2_actor=config[\"l2_actor\"],\n",
    "        l2_critic=config[\"l2_critic\"]\n",
    "    )\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 1\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_1.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Close Environment ---\n",
    "    print(\"Closing the environment...\")\n",
    "    env.close()\n",
    "    print(\"Environment closed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b191b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 2\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 2 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 2\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_2.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 5000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 3\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 3 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 3\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_3.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c425d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 3e-3,\n",
    "        \"critic_lr\": 3e-3,\n",
    "        \"num_episodes\": 5000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 4\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 4 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 4\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_4.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74daf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [30, 20, 40, 40],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 3e-3,\n",
    "        \"critic_lr\": 3e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "        \n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 5\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 5 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 5\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_5.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [60, 40, 80, 80],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 3e-3,\n",
    "        \"critic_lr\": 3e-3,\n",
    "        \"num_episodes\": 1000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 6\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 6 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 6\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_6.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2008ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [120, 80, 160, 160],\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 3e-3,\n",
    "        \"critic_lr\": 3e-3,\n",
    "        \"num_episodes\": 1000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 7\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 7 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 7\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_7.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1dd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.001,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [240, 160, 320, 320],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 2e-3,\n",
    "        \"critic_lr\": 2e-3,\n",
    "        \"num_episodes\": 500,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 8\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 8 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 8\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_8.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022032f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.001,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [480, 320, 640, 640],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 2e-3,\n",
    "        \"critic_lr\": 2e-3,\n",
    "        \"num_episodes\": 500,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 9\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 9 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 9\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_9.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aef7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.001,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [960, 640, 1280, 1280],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 1e-3,\n",
    "        \"critic_lr\": 1e-3,\n",
    "        \"num_episodes\": 300,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 10\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 10 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 10\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_10.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb56f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.001,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [1500, 1000, 2000, 2000],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": \"GRU_modified\",\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 1e-3,\n",
    "        \"critic_lr\": 1e-3,\n",
    "        \"num_episodes\": 250,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 25\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 11\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 11 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 11\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_rnn'\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / 'EC_P_rnn_11.pkl'\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cienciadatosenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
