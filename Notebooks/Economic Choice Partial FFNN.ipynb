{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29e4506",
   "metadata": {},
   "source": [
    "## Economic Choice Partial Environment FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95842b91",
   "metadata": {},
   "source": [
    "In this notebook, the FFNN agent is trained for the partially observed states environment. As learning is impossible for this agent, there is no increase in the number of time steps or modifications on the learning rate throughout the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- GPU Configuration ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# --- Add Modules directory to Python path ---\n",
    "module_path = os.path.abspath(os.path.join('..', 'Modules'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added '{module_path}' to sys.path\")\n",
    "else:\n",
    "    print(f\"'{module_path}' already in sys.path\")\n",
    "\n",
    "# --- Import custom modules ---\n",
    "from env_economic_choice_partial import EconomicChoiceEnv\n",
    "from actor_critic import ActorCriticAgent\n",
    "from reinforce import train_agent\n",
    "from helper_functions import load_model, save_model\n",
    "\n",
    "# --- Set Random Seeds ---\n",
    "seed_value = 1\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.0,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": 'Dense',\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, \\\n",
    "    actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config[\"num_episodes\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        print_interval=config[\"print_interval\"],\n",
    "        l2_actor=config[\"l2_actor\"],\n",
    "        l2_critic=config[\"l2_critic\"]\n",
    "    )\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 1\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    this_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_file = output_dir / 'EC_P_ffnn_1.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Close Environment ---\n",
    "    print(\"Closing the environment...\")\n",
    "    env.close()\n",
    "    print(\"Environment closed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d80721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": 'Dense',\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 2\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 2 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 2\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    this_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_file = output_dir / 'EC_P_ffnn_2.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": 'Dense',\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 3\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 3 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 3\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    this_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_file = output_dir / 'EC_P_ffnn_3.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": 'Dense',\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 10000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 4\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 4 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 4\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    this_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_file = output_dir / 'EC_P_ffnn_4.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c967305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    config = {\n",
    "        # Environment params\n",
    "        \"dt\": 10,\n",
    "        \"A_to_B_ratio\": 2.2,\n",
    "        \"reward_B\": 1,\n",
    "        \"abort_penalty\": -0.1,\n",
    "        \"input_noise_sigma\": 0.01,\n",
    "        \"reward_fixation\": 0.01,\n",
    "        \"reward_go_fixation\": -0.01,\n",
    "        \"duration_params\": [10, 10, 20, 20],\n",
    "\n",
    "        # Agent architecture\n",
    "        \"actor_hidden_size\": 50,\n",
    "        \"critic_hidden_size\": 50,\n",
    "        \"actor_layers\": 1,\n",
    "        \"critic_layers\": 1,\n",
    "        \"layer_type\": 'Dense',\n",
    "        \"actor_prob_connection\": 0.1,\n",
    "        \"critic_prob_connection\": 1.0,\n",
    "\n",
    "        # Training hyperparams\n",
    "        \"actor_lr\": 4e-3,\n",
    "        \"critic_lr\": 4e-3,\n",
    "        \"num_episodes\": 3550,\n",
    "        \"gamma\": 1.0,\n",
    "        \"l2_actor\": 1e-4,\n",
    "        \"l2_critic\": 1e-4,\n",
    "\n",
    "        # Training process\n",
    "        \"print_interval\": 100\n",
    "    }\n",
    "\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # --- Environment Setup ---\n",
    "    print(\"Creating Padoa-Schioppa environment...\")\n",
    "    env = EconomicChoiceEnv(\n",
    "        dt=config[\"dt\"],\n",
    "        A_to_B_ratio=config[\"A_to_B_ratio\"],\n",
    "        reward_B=config[\"reward_B\"],\n",
    "        abort_penalty=config[\"abort_penalty\"],\n",
    "        input_noise_sigma=config[\"input_noise_sigma\"],\n",
    "        reward_fixation=config[\"reward_fixation\"],\n",
    "        reward_go_fixation=config[\"reward_go_fixation\"],\n",
    "        duration_params=config[\"duration_params\"]\n",
    "    )\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "    print(f\"Observation size: {obs_size}, Action size: {act_size}\")\n",
    "\n",
    "    # --- Agent Setup ---\n",
    "    print(\"Initializing Actor-Critic Agent...\")\n",
    "    agent = ActorCriticAgent(\n",
    "        obs_size=obs_size,\n",
    "        act_size=act_size,\n",
    "        actor_hidden_size=config[\"actor_hidden_size\"],\n",
    "        critic_hidden_size=config[\"critic_hidden_size\"],\n",
    "        actor_layers=config[\"actor_layers\"],\n",
    "        critic_layers=config[\"critic_layers\"],\n",
    "        actor_lr=config[\"actor_lr\"],\n",
    "        critic_lr=config[\"critic_lr\"],\n",
    "        actor_prob_connection=config[\"actor_prob_connection\"],\n",
    "        critic_prob_connection=config[\"critic_prob_connection\"],\n",
    "        layer_type=config[\"layer_type\"]\n",
    "    )\n",
    "    print(\"Agent Initialized.\")\n",
    "\n",
    "    # --- Load checkpoint ---\n",
    "    this_stage = 5\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    load_model(agent, obs_size, act_size, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Training ---\n",
    "    print(f\"Starting training for {config['num_episodes']} episodes...\")\n",
    "    total_rewards_history, actor_loss_history, critic_loss_history, actor_firing_rates, critic_firing_rates, measurements_juices = train_agent(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=config['num_episodes'],\n",
    "        gamma=config['gamma'],\n",
    "        print_interval=config['print_interval'],\n",
    "        l2_actor=config['l2_actor'],\n",
    "        l2_critic=config['l2_critic']\n",
    "    )\n",
    "    print(\"Training stage 5 finished.\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    this_stage = 5\n",
    "    this_ckpt_dir = Path('..') / 'Saved' / 'Checkpoints' / 'EC_P_ffnn'\n",
    "    this_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_model(agent, this_stage, this_ckpt_dir)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path('..') / 'Saved' / 'Outputs'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_file = output_dir / 'EC_P_ffnn_5.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump((\n",
    "            total_rewards_history,\n",
    "            actor_loss_history,\n",
    "            critic_loss_history,\n",
    "            actor_firing_rates,\n",
    "            critic_firing_rates,\n",
    "            measurements_juices\n",
    "        ), f)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
